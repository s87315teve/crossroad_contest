import os
import cv2
import torch
import numpy as np
from ultralytics import YOLO
import pandas as pd
from datetime import datetime
import json
import time
from pathlib import Path
import multiprocessing
import shutil
import glob

class YOLOVideoAnalyzer:
    def __init__(self, recordings_root="recordings", model_path="yolo11n.pt"):
        """
        åˆå§‹åŒ– YOLO å½±ç‰‡åˆ†æå™¨
        
        Args:
            recordings_root: éŒ„è£½å½±ç‰‡çš„æ ¹ç›®éŒ„
            model_path: YOLO æ¨¡å‹è·¯å¾‘ (å¯ä½¿ç”¨ yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11l.pt, yolo11x.pt)
        """
        self.recordings_root = recordings_root
        self.model_path = model_path
        self.output_root = "yolo_analysis"
        self.model = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # å»ºç«‹è¼¸å‡ºç›®éŒ„
        os.makedirs(self.output_root, exist_ok=True)
        
        print(f"ä½¿ç”¨è¨­å‚™: {self.device}")
        
    def load_model(self):
        """è¼‰å…¥ YOLO æ¨¡å‹"""
        try:
            print(f"æ­£åœ¨è¼‰å…¥ YOLO æ¨¡å‹: {self.model_path}")
            # å¦‚æœæ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼Œæœƒè‡ªå‹•ä¸‹è¼‰
            self.model = YOLO(self.model_path)
            self.model.to(self.device)
            print("âœ“ YOLO æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âœ— è¼‰å…¥ YOLO æ¨¡å‹å¤±æ•—: {e}")
            return False
    
    def get_video_files(self):
        """æƒæéŒ„è£½ç›®éŒ„ï¼Œç²å–æ‰€æœ‰å½±ç‰‡æª”æ¡ˆ"""
        video_files = {}
        
        if not os.path.exists(self.recordings_root):
            print(f"éŒ„è£½ç›®éŒ„ä¸å­˜åœ¨: {self.recordings_root}")
            return video_files
        
        # æ”¯æ´çš„å½±ç‰‡æ ¼å¼
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']
        
        # éæ­·æ‰€æœ‰å­ç›®éŒ„ï¼ˆåœ°é»ï¼‰
        for location_dir in os.listdir(self.recordings_root):
            location_path = os.path.join(self.recordings_root, location_dir)
            
            if os.path.isdir(location_path):
                location_videos = []
                
                # ç²å–è©²åœ°é»çš„æ‰€æœ‰å½±ç‰‡æª”æ¡ˆ
                for ext in video_extensions:
                    pattern = os.path.join(location_path, f"*{ext}")
                    location_videos.extend(glob.glob(pattern))
                
                if location_videos:
                    # æŒ‰æª”æ¡ˆä¿®æ”¹æ™‚é–“æ’åº
                    location_videos.sort(key=lambda x: os.path.getmtime(x))
                    video_files[location_dir] = location_videos
                    print(f"æ‰¾åˆ° {location_dir}: {len(location_videos)} å€‹å½±ç‰‡æª”æ¡ˆ")
        
        return video_files
    
    def analyze_single_video(self, video_path, location):
        """åˆ†æå–®ä¸€å½±ç‰‡æª”æ¡ˆ"""
        print(f"é–‹å§‹åˆ†æå½±ç‰‡: {video_path}")
        
        # å»ºç«‹è©²åœ°é»çš„è¼¸å‡ºç›®éŒ„
        location_output_dir = os.path.join(self.output_root, location)
        os.makedirs(location_output_dir, exist_ok=True)
        
        # å»ºç«‹å­ç›®éŒ„
        frames_dir = os.path.join(location_output_dir, "detected_frames")
        videos_dir = os.path.join(location_output_dir, "annotated_videos")
        data_dir = os.path.join(location_output_dir, "detection_data")
        
        os.makedirs(frames_dir, exist_ok=True)
        os.makedirs(videos_dir, exist_ok=True)
        os.makedirs(data_dir, exist_ok=True)
        
        # ç²å–å½±ç‰‡æª”åï¼ˆä¸å«è·¯å¾‘å’Œå‰¯æª”åï¼‰
        video_filename = Path(video_path).stem
        
        try:
            # é–‹å•Ÿå½±ç‰‡
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {video_path}")
                return
            
            # ç²å–å½±ç‰‡å±¬æ€§
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            print(f"å½±ç‰‡è³‡è¨Š - è§£æåº¦: {width}x{height}, FPS: {fps}, ç¸½å¹€æ•¸: {total_frames}")
            
            # å»ºç«‹è¼¸å‡ºå½±ç‰‡å¯«å…¥å™¨
            output_video_path = os.path.join(videos_dir, f"{video_filename}_annotated.mp4")
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
            
            # å„²å­˜åµæ¸¬çµæœçš„åˆ—è¡¨
            detections_data = []
            frame_count = 0
            detection_count = 0
            
            print(f"é–‹å§‹è™•ç†å½±ç‰‡å¹€...")
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # ä½¿ç”¨ YOLO é€²è¡Œåµæ¸¬
                results = self.model(frame, verbose=False)
                
                # è™•ç†åµæ¸¬çµæœ
                frame_detections = []
                annotated_frame = frame.copy()
                
                for result in results:
                    boxes = result.boxes
                    if boxes is not None:
                        for box in boxes:
                            # ç²å–é‚Šç•Œæ¡†åº§æ¨™
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            confidence = box.conf[0].cpu().numpy()
                            class_id = int(box.cls[0].cpu().numpy())
                            class_name = self.model.names[class_id]
                            
                            # åªä¿å­˜ä¿¡å¿ƒåº¦å¤§æ–¼ 0.5 çš„åµæ¸¬çµæœ
                            if confidence > 0.5:
                                detection_count += 1
                                
                                # è¨˜éŒ„åµæ¸¬è³‡æ–™
                                detection_info = {
                                    'frame_number': frame_count,
                                    'timestamp': frame_count / fps,  # ç§’æ•¸
                                    'class_name': class_name,
                                    'confidence': float(confidence),
                                    'bbox': [float(x1), float(y1), float(x2), float(y2)]
                                }
                                frame_detections.append(detection_info)
                                
                                # åœ¨å½±ç‰‡å¹€ä¸Šç¹ªè£½é‚Šç•Œæ¡†å’Œæ¨™ç±¤
                                cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                                label = f"{class_name}: {confidence:.2f}"
                                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                                cv2.rectangle(annotated_frame, (int(x1), int(y1) - label_size[1] - 10), 
                                            (int(x1) + label_size[0], int(y1)), (0, 255, 0), -1)
                                cv2.putText(annotated_frame, label, (int(x1), int(y1) - 5), 
                                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
                
                # å¦‚æœæœ‰åµæ¸¬åˆ°ç‰©ä»¶ï¼Œå„²å­˜è©²å¹€
                if frame_detections:
                    detections_data.extend(frame_detections)
                    
                    # æ¯10å¹€æˆ–æœ‰é‡è¦åµæ¸¬æ™‚å„²å­˜é—œéµå¹€
                    if frame_count % 30 == 0 or len(frame_detections) > 3:  # æ¯ç§’æˆ–å¤šå€‹ç‰©ä»¶æ™‚å„²å­˜
                        frame_filename = f"{video_filename}_frame_{frame_count:06d}.jpg"
                        frame_path = os.path.join(frames_dir, frame_filename)
                        cv2.imwrite(frame_path, annotated_frame)
                
                # å¯«å…¥æ¨™è¨»å¾Œçš„å½±ç‰‡
                out.write(annotated_frame)
                
                # é¡¯ç¤ºè™•ç†é€²åº¦
                if frame_count % 100 == 0:
                    progress = (frame_count / total_frames) * 100
                    print(f"{location} - è™•ç†é€²åº¦: {progress:.1f}% ({frame_count}/{total_frames}), åµæ¸¬æ•¸: {detection_count}")
            
            # æ¸…ç†è³‡æº
            cap.release()
            out.release()
            
            # å„²å­˜åµæ¸¬è³‡æ–™ç‚º JSON å’Œ CSV
            if detections_data:
                # JSON æ ¼å¼
                json_path = os.path.join(data_dir, f"{video_filename}_detections.json")
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(detections_data, f, indent=2, ensure_ascii=False)
                
                # CSV æ ¼å¼
                csv_path = os.path.join(data_dir, f"{video_filename}_detections.csv")
                df = pd.DataFrame(detections_data)
                df.to_csv(csv_path, index=False, encoding='utf-8')
                
                # å»ºç«‹åµæ¸¬æ‘˜è¦
                summary = self.create_detection_summary(detections_data, video_filename, total_frames, fps)
                summary_path = os.path.join(data_dir, f"{video_filename}_summary.json")
                with open(summary_path, 'w', encoding='utf-8') as f:
                    json.dump(summary, f, indent=2, ensure_ascii=False)
                
                print(f"âœ“ å®Œæˆåˆ†æ: {video_path}")
                print(f"  - ç¸½åµæ¸¬æ•¸: {detection_count}")
                print(f"  - è¼¸å‡ºå½±ç‰‡: {output_video_path}")
                print(f"  - åµæ¸¬è³‡æ–™: {json_path}")
                print(f"  - æ‘˜è¦è³‡æ–™: {summary_path}")
            else:
                print(f"âœ“ å®Œæˆåˆ†æ: {video_path} (æœªåµæ¸¬åˆ°ç‰©ä»¶)")
        
        except Exception as e:
            print(f"åˆ†æå½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤ ({video_path}): {e}")
    
    def create_detection_summary(self, detections_data, video_filename, total_frames, fps):
        """å»ºç«‹åµæ¸¬çµæœæ‘˜è¦"""
        if not detections_data:
            return {}
        
        df = pd.DataFrame(detections_data)
        
        # åŸºæœ¬çµ±è¨ˆ
        class_counts = df['class_name'].value_counts().to_dict()
        avg_confidence = df['confidence'].mean()
        max_confidence = df['confidence'].max()
        min_confidence = df['confidence'].min()
        
        # æ™‚é–“çµ±è¨ˆ
        detection_duration = df['timestamp'].max() - df['timestamp'].min()
        detection_frames = len(df['frame_number'].unique())
        
        summary = {
            'video_filename': video_filename,
            'analysis_time': datetime.now().isoformat(),
            'video_stats': {
                'total_frames': total_frames,
                'fps': fps,
                'duration_seconds': total_frames / fps
            },
            'detection_stats': {
                'total_detections': len(detections_data),
                'unique_frames_with_detection': detection_frames,
                'detection_coverage_percentage': (detection_frames / total_frames) * 100,
                'detection_duration_seconds': float(detection_duration),
                'class_counts': class_counts,
                'confidence_stats': {
                    'average': float(avg_confidence),
                    'maximum': float(max_confidence),
                    'minimum': float(min_confidence)
                }
            }
        }
        
        return summary
    
    def analyze_location_videos(self, location, video_files):
        """åˆ†ææŸå€‹åœ°é»çš„æ‰€æœ‰å½±ç‰‡"""
        print(f"\né–‹å§‹åˆ†æåœ°é»: {location} ({len(video_files)} å€‹å½±ç‰‡)")
        
        for i, video_path in enumerate(video_files, 1):
            print(f"\n[{i}/{len(video_files)}] è™•ç† {location} çš„å½±ç‰‡...")
            self.analyze_single_video(video_path, location)
    
    def start_analysis(self, parallel=False, max_workers=None):
        """é–‹å§‹åˆ†ææ‰€æœ‰å½±ç‰‡"""
        if not self.load_model():
            return
        
        # ç²å–æ‰€æœ‰å½±ç‰‡æª”æ¡ˆ
        video_files = self.get_video_files()
        
        if not video_files:
            print("æ²’æœ‰æ‰¾åˆ°å½±ç‰‡æª”æ¡ˆ")
            return
        
        print(f"\næ‰¾åˆ° {len(video_files)} å€‹åœ°é»çš„å½±ç‰‡")
        
        if parallel and len(video_files) > 1:
            # å¹³è¡Œè™•ç†å¤šå€‹åœ°é»
            if max_workers is None:
                max_workers = min(len(video_files), multiprocessing.cpu_count())
            
            print(f"ä½¿ç”¨å¹³è¡Œè™•ç†ï¼Œå·¥ä½œé€²ç¨‹æ•¸: {max_workers}")
            
            with multiprocessing.Pool(max_workers) as pool:
                tasks = [(location, videos) for location, videos in video_files.items()]
                pool.starmap(self.analyze_location_videos, tasks)
        else:
            # åºåˆ—è™•ç†
            for location, videos in video_files.items():
                self.analyze_location_videos(location, videos)
        
        print("\nğŸ‰ æ‰€æœ‰å½±ç‰‡åˆ†æå®Œæˆ!")
        self.generate_overall_report(video_files)
    
    def generate_overall_report(self, video_files):
        """ç”¢ç”Ÿæ•´é«”åˆ†æå ±å‘Š"""
        print("\næ­£åœ¨ç”¢ç”Ÿæ•´é«”å ±å‘Š...")
        
        overall_stats = {
            'analysis_time': datetime.now().isoformat(),
            'total_locations': len(video_files),
            'total_videos': sum(len(videos) for videos in video_files.values()),
            'locations': {}
        }
        
        for location in video_files.keys():
            location_dir = os.path.join(self.output_root, location, "detection_data")
            if os.path.exists(location_dir):
                summary_files = glob.glob(os.path.join(location_dir, "*_summary.json"))
                
                location_stats = {
                    'video_count': len(summary_files),
                    'total_detections': 0,
                    'class_distribution': {}
                }
                
                for summary_file in summary_files:
                    try:
                        with open(summary_file, 'r', encoding='utf-8') as f:
                            summary = json.load(f)
                            
                        detection_stats = summary.get('detection_stats', {})
                        location_stats['total_detections'] += detection_stats.get('total_detections', 0)
                        
                        class_counts = detection_stats.get('class_counts', {})
                        for class_name, count in class_counts.items():
                            location_stats['class_distribution'][class_name] = \
                                location_stats['class_distribution'].get(class_name, 0) + count
                    
                    except Exception as e:
                        print(f"è®€å–æ‘˜è¦æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ ({summary_file}): {e}")
                
                overall_stats['locations'][location] = location_stats
        
        # å„²å­˜æ•´é«”å ±å‘Š
        report_path = os.path.join(self.output_root, "overall_analysis_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(overall_stats, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ æ•´é«”å ±å‘Šå·²å„²å­˜: {report_path}")

def main():
    """ä¸»ç¨‹å¼"""
    print("=== YOLOv11 å½±ç‰‡åˆ†æç³»çµ± ===")
    
    # è¨­å®šåƒæ•¸
    recordings_dir = "recordings"  # éŒ„è£½å½±ç‰‡çš„ç›®éŒ„
    model_name = "yolo12n.pt"     # å¯é¸: yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11l.pt, yolo11x.pt
    
    # æª¢æŸ¥éŒ„è£½ç›®éŒ„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(recordings_dir):
        print(f"éŒ„è£½ç›®éŒ„ä¸å­˜åœ¨: {recordings_dir}")
        print("è«‹ç¢ºä¿å·²ç¶“åŸ·è¡Œéå½±ç‰‡çˆ¬å–ç¨‹å¼ï¼Œä¸¦ä¸”æœ‰éŒ„è£½çš„å½±ç‰‡æª”æ¡ˆã€‚")
        return
    
    # å»ºç«‹åˆ†æå™¨
    analyzer = YOLOVideoAnalyzer(recordings_dir, model_name)
    
    # è©¢å•ä½¿ç”¨è€…æ˜¯å¦è¦å¹³è¡Œè™•ç†
    use_parallel = input("æ˜¯å¦ä½¿ç”¨å¹³è¡Œè™•ç†ï¼Ÿ(y/n, é è¨­: n): ").lower().strip() == 'y'
    
    if use_parallel:
        max_workers = input(f"è¼¸å…¥å·¥ä½œé€²ç¨‹æ•¸ (1-{multiprocessing.cpu_count()}, é è¨­: è‡ªå‹•): ").strip()
        max_workers = int(max_workers) if max_workers.isdigit() else None
    else:
        max_workers = None
    
    print(f"\nä½¿ç”¨æ¨¡å‹: {model_name}")
    print(f"è™•ç†æ¨¡å¼: {'å¹³è¡Œè™•ç†' if use_parallel else 'åºåˆ—è™•ç†'}")
    print(f"è¼¸å…¥ç›®éŒ„: {recordings_dir}")
    print(f"è¼¸å‡ºç›®éŒ„: yolo_analysis")
    
    input("\næŒ‰ Enter é–‹å§‹åˆ†æ...")
    
    # é–‹å§‹åˆ†æ
    start_time = time.time()
    analyzer.start_analysis(parallel=use_parallel, max_workers=max_workers)
    end_time = time.time()
    
    print(f"\nç¸½è™•ç†æ™‚é–“: {end_time - start_time:.2f} ç§’")

if __name__ == "__main__":
    # Windows ç›¸å®¹æ€§è¨­å®š
    if hasattr(multiprocessing, 'set_start_method'):
        multiprocessing.set_start_method('spawn', force=True)
    
    main()